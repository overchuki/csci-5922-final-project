{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchtune\n",
    "import pickle as pkl\n",
    "\n",
    "from typing import Literal\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import v2\n",
    "from torchsummary import summary\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall, MulticlassF1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIM = 128\n",
    "NUM_CHANNELS = 3\n",
    "BATCH_SIZE = 512\n",
    "NORMALIZE_MEAN = (0.485,0.456,0.406)\n",
    "NORMALIZE_STD = (0.229,0.224,0.225)\n",
    "NUM_CLASSES = 90\n",
    "NUM_REAL_IMG_PER_CLASS = 60\n",
    "NUM_AI_IMG_PER_CLASS = 30\n",
    "REAL_IMG_TRAIN_PERCENTAGE = 0.5\n",
    "REAL_IMG_TEST_PERCENTAGE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset_indices(\n",
    "    num_img_per_class: int,\n",
    "    percent: float = 1.0,\n",
    "    side: Literal[\"left\", \"right\"] = \"left\",\n",
    ") -> np.ndarray:\n",
    "    indices = []\n",
    "    for i in range(NUM_CLASSES):\n",
    "        base = i * num_img_per_class\n",
    "        class_size = int(num_img_per_class * percent)\n",
    "        start = 0 if side == \"left\" else num_img_per_class - class_size\n",
    "        indices.extend(\n",
    "            list(np.arange(base + start, base + start + class_size))\n",
    "        )\n",
    "    return np.array(indices, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(\n",
    "    real_img_dir: str = \"./real_animals\",\n",
    "    ai_img_dir: str = \"./ai_animals\",\n",
    "    real_img_percent: float = 1.0,\n",
    "    ai_img_percent: float = 0.0,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    num_workers: int = 0,\n",
    "    shuffle: bool = True,\n",
    ") -> tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n",
    "    \"\"\"\n",
    "    Get train/test dataloaders for real and AI-generated images.\n",
    "    \"\"\"\n",
    "    transform = v2.Compose([\n",
    "        v2.Resize((IMG_DIM, IMG_DIM)),\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean=NORMALIZE_MEAN, std=NORMALIZE_STD),\n",
    "    ])\n",
    "\n",
    "    train_real_img_subset = torch.utils.data.Subset(\n",
    "        torchvision.datasets.ImageFolder(\n",
    "            root=real_img_dir,\n",
    "            transform=transform,\n",
    "            allow_empty=True,\n",
    "        ),\n",
    "        indices=get_subset_indices(NUM_REAL_IMG_PER_CLASS, percent=REAL_IMG_TRAIN_PERCENTAGE * real_img_percent, side=\"left\"),\n",
    "    )\n",
    "    test_real_img_subset = torch.utils.data.Subset(\n",
    "        torchvision.datasets.ImageFolder(\n",
    "            root=real_img_dir,\n",
    "            transform=transform,\n",
    "            allow_empty=True,\n",
    "        ),\n",
    "        indices=get_subset_indices(NUM_REAL_IMG_PER_CLASS, percent=REAL_IMG_TEST_PERCENTAGE, side=\"right\"),\n",
    "    )\n",
    "    ai_img_subset = torch.utils.data.Subset(\n",
    "        torchvision.datasets.ImageFolder(\n",
    "            root=ai_img_dir,\n",
    "            transform=transform,\n",
    "            allow_empty=True,\n",
    "        ),\n",
    "        indices=get_subset_indices(NUM_AI_IMG_PER_CLASS, percent=ai_img_percent, side=\"left\"),\n",
    "    )\n",
    "\n",
    "    train_dataset = torchtune.datasets.ConcatDataset(datasets=[train_real_img_subset, ai_img_subset]) if ai_img_percent > 0.0 else train_real_img_subset\n",
    "    train_img_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    test_img_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=test_real_img_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return train_img_dataloader, test_img_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl = get_loader(real_img_percent=1.0, ai_img_percent=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:50<00:00,  4.60s/it]\n"
     ]
    }
   ],
   "source": [
    "for img, label in tqdm(train_dl, total=len(train_dl)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:50<00:00,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{54: 60, 63: 60, 64: 60, 89: 60, 33: 60, 51: 60, 25: 60, 77: 60, 19: 60, 41: 60, 22: 60, 49: 60, 74: 60, 78: 60, 17: 60, 79: 60, 86: 60, 23: 60, 50: 60, 73: 60, 52: 60, 32: 60, 26: 60, 31: 60, 76: 60, 60: 60, 45: 60, 46: 60, 35: 60, 67: 60, 12: 60, 75: 60, 37: 60, 55: 60, 62: 60, 7: 60, 5: 60, 40: 60, 6: 60, 47: 60, 30: 60, 65: 60, 53: 60, 85: 60, 71: 60, 3: 60, 83: 60, 43: 60, 72: 60, 10: 60, 61: 60, 36: 60, 27: 60, 87: 60, 21: 60, 8: 60, 20: 60, 84: 60, 9: 60, 70: 60, 81: 60, 13: 60, 42: 60, 82: 60, 88: 60, 34: 60, 15: 60, 28: 60, 44: 60, 66: 60, 29: 60, 48: 60, 58: 60, 1: 60, 0: 60, 68: 60, 59: 60, 4: 60, 18: 60, 16: 60, 69: 60, 24: 60, 11: 60, 39: 60, 56: 60, 2: 60, 80: 60, 14: 60, 38: 60, 57: 60}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "class_counts = {}\n",
    "for img, label in tqdm(train_dl, total=len(train_dl)):\n",
    "    i += 1\n",
    "    # if i > 4:\n",
    "    #     break\n",
    "    # print(img.shape, label.shape)\n",
    "    for l in label:\n",
    "        if l.item() not in class_counts:\n",
    "            class_counts[l.item()] = 0\n",
    "        class_counts[l.item()] += 1\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channels, n_classes):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # set metadata\n",
    "        self.input_channels = input_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.FINAL_LAYER_SIZE = 4\n",
    "        self.final_layer_channels = 40\n",
    "        self.flatten_layer_size = self.final_layer_channels * self.FINAL_LAYER_SIZE * self.FINAL_LAYER_SIZE\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout50 = nn.Dropout(p=0.5)\n",
    "        self.dropout10 = nn.Dropout(p=0.1)\n",
    "\n",
    "        # set up layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=8, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=24, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(in_channels=24, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv5 = nn.Conv2d(in_channels=32, out_channels=self.final_layer_channels, kernel_size=3, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # self.conv6 = nn.Conv2d(in_channels=32, out_channels=self.final_layer_channels, kernel_size=3, padding=1)\n",
    "        # self.pool6 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(self.flatten_layer_size, 128)\n",
    "        self.fc2 = nn.Linear(128, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1: conv -> pool\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout10(torch.nn.functional.leaky_relu(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # 2: conv -> pool\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout10(torch.nn.functional.leaky_relu(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # 3: conv -> pool\n",
    "        x = self.conv3(x)\n",
    "        x = self.dropout10(torch.nn.functional.leaky_relu(x))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # 4: conv -> pool\n",
    "        x = self.conv4(x)\n",
    "        x = self.dropout10(torch.nn.functional.leaky_relu(x))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        # 5: conv -> pool\n",
    "        x = self.conv5(x)\n",
    "        x = self.dropout10(torch.nn.functional.leaky_relu(x))\n",
    "        x = self.pool5(x)\n",
    "\n",
    "        # # 6: conv -> pool\n",
    "        # x = self.conv6(x)\n",
    "        # x = self.dropout10(torch.nn.functional.leaky_relu(x))\n",
    "        # x = self.pool6(x)\n",
    "\n",
    "        # flatten the features (the first dimension is batch size)\n",
    "        x = x.view(-1, self.flatten_layer_size)\n",
    "\n",
    "        # fc layers\n",
    "        x = self.dropout50(torch.nn.functional.leaky_relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [512, 8, 128, 128]             224\n",
      "           Dropout-2         [512, 8, 128, 128]               0\n",
      "         MaxPool2d-3           [512, 8, 64, 64]               0\n",
      "            Conv2d-4          [512, 16, 64, 64]           1,168\n",
      "           Dropout-5          [512, 16, 64, 64]               0\n",
      "         MaxPool2d-6          [512, 16, 32, 32]               0\n",
      "            Conv2d-7          [512, 24, 32, 32]           3,480\n",
      "           Dropout-8          [512, 24, 32, 32]               0\n",
      "         MaxPool2d-9          [512, 24, 16, 16]               0\n",
      "           Conv2d-10          [512, 32, 16, 16]           6,944\n",
      "          Dropout-11          [512, 32, 16, 16]               0\n",
      "        MaxPool2d-12            [512, 32, 8, 8]               0\n",
      "           Conv2d-13            [512, 40, 8, 8]          11,560\n",
      "          Dropout-14            [512, 40, 8, 8]               0\n",
      "        MaxPool2d-15            [512, 40, 4, 4]               0\n",
      "           Linear-16                 [512, 128]          82,048\n",
      "          Dropout-17                 [512, 128]               0\n",
      "           Linear-18                  [512, 90]          11,610\n",
      "================================================================\n",
      "Total params: 117,034\n",
      "Trainable params: 117,034\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 96.00\n",
      "Forward/backward pass size (MB): 2039.85\n",
      "Params size (MB): 0.45\n",
      "Estimated Total Size (MB): 2136.30\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(CNN(input_channels=3, n_classes=NUM_CLASSES).to(device), (3, IMG_DIM, IMG_DIM), batch_size=BATCH_SIZE, device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedResNet50(nn.Module):\n",
    "    def __init__(self, input_channels, n_classes):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # set metadata\n",
    "        self.input_channels = input_channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(CombinedResNet50(input_channels=3, n_classes=NUM_CLASSES).to(device), (3, IMG_DIM, IMG_DIM), batch_size=BATCH_SIZE, device=device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(results) -> None:\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16, 4))\n",
    "    fig.subplots_adjust(wspace=0.4)\n",
    "\n",
    "    # ax[0].set_title(\"Loss\")\n",
    "    # ax[0].set_xlabel(\"Epoch\")\n",
    "    # ax[0].plot(results[0][0], label=\"Train Loss\", color=\"blue\")\n",
    "    # ax[0].plot(results[1][0], label=\"Test Loss\", color=\"red\")\n",
    "    # ax[0].legend()\n",
    "    # ax[0].set_ylabel(\"Cross Entropy Loss\")\n",
    "\n",
    "    # ax[1].set_title(\"Accuracy\")\n",
    "    # ax[1].set_xlabel(\"Epoch\")\n",
    "    # ax[1].plot(results[0][1], label=\"Train Accuracy\", color=\"blue\")\n",
    "    # ax[1].plot(results[1][1], label=\"Test Accuracy\", color=\"red\")\n",
    "    # ax[1].legend()\n",
    "    # ax[1].set_ylabel(\"Accuracy (%)\")\n",
    "\n",
    "    # ax[2].set_title(\"Precision and Recall\")\n",
    "    # ax[2].set_xlabel(\"Epoch\")\n",
    "    # ax[2].plot(results[2][0], label=\"Test Precision\", color=\"cyan\")\n",
    "    # ax[2].plot(results[2][1], label=\"Test Recall\", color=\"orange\")\n",
    "    # ax[2].legend()\n",
    "    # ax[2].set_ylabel(\"Precision/Recall Score (0-1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_improvement_by_class(results) -> None:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diff_barchart(results) -> None:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(\n",
    "        model: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        train_loader: torch.utils.data.DataLoader,\n",
    "        test_loader: torch.utils.data.DataLoader,\n",
    "        E: int,\n",
    "        verbose: Literal[\"none\", \"prints\", \"epoch_tqdm\", \"loader_tqdm\"] = \"epoch_tqdm\",\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Train and test the given model with the given parameters.\n",
    "    \"\"\"\n",
    "    # model = torch.compile(model)\n",
    "    loss_function = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    accuracy_metric = MulticlassAccuracy(average='none', num_classes=NUM_CLASSES).to(device)\n",
    "    precision_metric = MulticlassPrecision(average='none', num_classes=NUM_CLASSES).to(device)\n",
    "    recall_metric = MulticlassRecall(average='none', num_classes=NUM_CLASSES).to(device)\n",
    "    f1_metric = MulticlassF1Score(average='none', num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "    train_losses = []\n",
    "    avg_train_accuracies = []\n",
    "    test_losses = []\n",
    "    test_accuracies = 0\n",
    "    avg_test_accuracies = []\n",
    "    test_precision = 0\n",
    "    test_recall = 0\n",
    "    test_f1score = 0\n",
    "    avg_test_precision = 0\n",
    "    avg_test_recall = 0\n",
    "    avg_test_f1score = 0\n",
    "\n",
    "    for epoch in tqdm(range(E), total=E, disable=verbose!=\"epoch_tqdm\"):\n",
    "        # TRAINING\n",
    "        model.train()\n",
    "        batch_losses = []\n",
    "        accuracy_metric.reset()\n",
    "        for images, labels in tqdm(train_loader, total=len(train_loader), disable=verbose!=\"loader_tqdm\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_losses.append(loss.item())\n",
    "            accuracy_metric.update(outputs, labels)\n",
    "        train_loss = np.mean(np.array(batch_losses))\n",
    "        train_losses.append(train_loss)\n",
    "        train_acc = accuracy_metric.compute()\n",
    "        avg_train_accuracies.append(train_acc.mean().item())\n",
    "\n",
    "        # TESTING\n",
    "        model.eval()\n",
    "        test_batch_losses = []\n",
    "        accuracy_metric.reset()\n",
    "        precision_metric.reset()\n",
    "        recall_metric.reset()\n",
    "        for images, labels in tqdm(test_loader, total=len(test_loader), disable=verbose!=\"loader_tqdm\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            test_batch_losses.append(loss_function(outputs, labels).item())\n",
    "            accuracy_metric.update(outputs, labels)\n",
    "            if epoch >= E - 1:\n",
    "                precision_metric.update(outputs, labels)\n",
    "                recall_metric.update(outputs, labels)\n",
    "                f1_metric.update(outputs, labels)\n",
    "        test_loss = np.mean(np.array(test_batch_losses))\n",
    "        test_losses.append(test_loss)\n",
    "        test_acc = accuracy_metric.compute()\n",
    "        avg_test_accuracies.append(test_acc.mean().item())\n",
    "        if epoch >= E - 1:\n",
    "            test_accuracies = test_acc.cpu().numpy()\n",
    "            test_precision = precision_metric.compute().cpu().numpy()\n",
    "            test_recall = recall_metric.compute().cpu().numpy()\n",
    "            test_f1score = f1_metric.compute().cpu().numpy()\n",
    "            avg_test_precision = test_precision.mean().item()\n",
    "            avg_test_recall = test_recall.mean().item()\n",
    "            avg_test_f1score = test_f1score.mean().item()\n",
    "\n",
    "        if verbose==\"prints\":\n",
    "            print(f\"Epoch [{epoch+1}/{E}]: Train Accuracy: {avg_train_accuracies[-1]*100:.2f}%, Train Loss: {train_loss:.4f}, Test Accuracy: {avg_test_accuracies[-1]*100:.2f}%, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    print(f\"\\nEvaluation results:\\nTrain Accuracy: {avg_train_accuracies[-1]*100:.2f}%, Train Loss: {train_loss:.4f}\\nTest Accuracy: {avg_test_accuracies[-1]*100:.2f}%, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"avg_train_accuracies\": avg_train_accuracies,\n",
    "        \"test_losses\": test_losses,\n",
    "        \"test_accuracies\": test_accuracies,\n",
    "        \"avg_test_accuracies\": avg_test_accuracies,\n",
    "        \"test_precision\": test_precision,\n",
    "        \"test_recall\": test_recall,\n",
    "        \"test_f1score\": test_f1score,\n",
    "        \"avg_test_precision\": avg_test_precision,\n",
    "        \"avg_test_recall\": avg_test_recall,\n",
    "        \"avg_test_f1score\": avg_test_f1score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_loader(real_img_percent=1.0, ai_img_percent=0.0, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:54<00:00, 54.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation results:\n",
      "Train Accuracy: 1.11%, Train Loss: 4.5019\n",
      "Test Accuracy: 1.11%, Test Loss: 4.4992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = CNN(input_channels=NUM_CHANNELS, n_classes=NUM_CLASSES).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "results_1 = train_and_test_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    E=1,\n",
    "    verbose=\"epoch_tqdm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results_1.pkl\", \"wb\") as f:\n",
    "    pkl.dump(results_1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
